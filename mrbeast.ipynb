{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689,
          "referenced_widgets": [
            "0d936312776a4d0dbc11c0db4e2ff367",
            "4d11d0e8a4ee489f878900fb9f704371",
            "7d6fec8b54714050aa718017c555965f",
            "6b8c8771ca084a2bb429546536860fb3",
            "06f581b2203f474da921362f6e0d1361",
            "4e06d33a4b7c4d5b819653a1c650f9a1",
            "96a913ae82c743ad9bfb906fed2511dd",
            "fc8d06f2f9da418aa7e4faf827cf5527",
            "482780a4246e40299d15400387080852",
            "d5be40e3220d44298119eb976fbace48",
            "745214b4123645ea835ed51f090718ce"
          ]
        },
        "id": "KBkZk5Mx2_kO",
        "outputId": "fa1557ca-abfe-4df3-8261-29873afc5b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 122\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-6803zl35wka2 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true --log_code_content')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d936312776a4d0dbc11c0db4e2ff367"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcR990Xu3ZfK",
        "outputId": "84c5bbdf-123e-419c-e342-57fa0f983a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0BUEAQG5rx9",
        "outputId": "6f1c9482-f95f-4c91-c905-1f583ffa2a3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
              " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids\n",
        "\n",
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw32LX4f5wSY"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"out.txt\")\n",
        "documents = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmFVtwTj6Keh"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DmNkpRd_YZT",
        "outputId": "9f1bde82-ebb2-4fbf-90f1-42ab0988e6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "lui9uj4TMXqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.7,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "ueTr1F1WNvhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain me what Mr.Beast does\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9vasHUTNyAc",
        "outputId": "dca32e0d-5d72-470c-a777-08800870b7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain me what Mr.Beast does?\n",
            "MrBeast, whose real name is Jimmy Donaldson, is a popular American YouTuber and philanthropist known for his generous acts of kindness and large-scale charity giveaways. He has gained a massive following on YouTube and other social media platforms due to his outrageous stunts, challenges, and donations to various causes.\n",
            "\n",
            "Here are some examples of what MrBeast does:\n",
            "\n",
            "1. Large-scale charity giveaways: MrBeast is famous for organizing and funding large-scale charity giveaways, often involving thousands or even millions of dollars in donations. These giveaways can range from paying off mortgages for struggling homeowners to buying new cars for families in need.\n",
            "2. Outrageous stunts and challenges: MrBeast is known for performing outrageous stunts and challenges, such as eating 100 hot peppers in one sitting, jumping into a pool filled with sharks, or attempting to break world records. These stunts often go viral and help to increase his popularity.\n",
            "3. Vlogs and comedy skits: In addition to his charity work and stunts, MrBeast also creates vlogs and comedy skits that showcase his humor and personality. These videos often feature him doing ridiculous things, like pretending to be a superhero or trying to eat an entire pizza by himself.\n",
            "4. Collaborations with other creators: MrBeast frequently collaborates with other popular YouTubers and influencers, creating content that combines their unique talents and senses of humor. These collaborations often result in hilarious and entertaining videos that his fans love.\n",
            "5. Philanthropy: MrBeast's primary focus is on giving back to the community through his charitable efforts. He has donated millions of dollars to various causes, including cancer research, homeless shelters, and disaster relief efforts.\n",
            "\n",
            "Overall, MrBeast is a unique and entertaining figure who has captured the hearts of millions of people around the world. His combination of generosity, humor, and creativity has made him one of the most popular and beloved figures on social media today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Explain me what Mr.Beast does\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "BKVxKuHUN1LO",
        "outputId": "0b629498-a004-4547-e09f-5f1ce7089883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" in his videos?\\nMrBeast, whose real name is Jimmy Donaldson, is a popular YouTuber known for his eccentric and generous acts of kindness. In his videos, he often performs outrageous stunts, gives away large sums of money to random people, or engages in other forms of charity work. He has gained a massive following on YouTube due to his over-the-top antics and generosity. Some common themes in his videos include:\\n\\n1. Giveaways: MrBeast frequently gives away large sums of money to random people, often in unexpected ways. For example, he might surprise someone by giving them $10,000 cash or a new car.\\n2. Stunts: MrBeast loves to perform outrageous stunts, such as jumping off a cliff into a lake, attempting to break world records, or doing crazy challenges like eating 100 hot wings in one sitting.\\n3. Charity work: MrBeast is known for his philanthropic efforts, often donating millions of dollars to various causes, such as disaster relief, medical expenses, or supporting small businesses.\\n4. Challenges: MrBeast issues challenges to himself or others, often involving dangerous or difficult tasks that must be completed within a set time frame.\\n5. Pranks: MrBeast enjoys playing pranks on his friends, family, and even strangers, often resulting in hilarious and unexpected consequences.\\n6. Travel vlogs: MrBeast frequently travels to different countries and documents his experiences through vlogs, showcasing the culture, food, and attractions of each place.\\n7. Collaborations: MrBeast often collaborates with other YouTubers, celebrities, or influencers, creating unique and entertaining content along the way.\\n8. Contests: MrBeast hosts contests and giveaways, inviting viewers to participate and win prizes ranging from small gifts to large sums of money.\\n9. Music videos: MrBeast occasionally creates and releases music videos, often featuring himself rapping or singing about various topics.\\n10. Personal stories: MrBeast shares personal stories and experiences, providing insight into his life and motivations behind his actions.\\n\\nOverall, MrBeast'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ],
      "metadata": {
        "id": "x8pKPINXOe9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "query = \"What does mr.beast do for a living and what kinda person he is\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHAo47AxOs2O",
        "outputId": "a23cc108-9b8b-46fd-8a42-0c6e9496ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mr. Beast is a popular YouTuber known for his generous acts of charity and giveaways. He has a large following on social media and uses his platform to raise money for various causes, such as buying houses for families in need, funding scholarships, and providing financial support to individuals who are experiencing hardship. Based on the text provided, it seems that Mr. Beast is a kind and generous person who is passionate about helping others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "query = \"give an example where he might have helped someone\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jXow_uaQBa7",
        "outputId": "3a3cff5c-3cbc-443d-daab-000acc5831f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Of course! One notable example is when Mr. Beast surprised a terminally ill cancer patient named Tyler with a life-changing donation of $30,000. Tyler had been diagnosed with kidney cancer at the age of 19 and had already gone through multiple rounds of chemotherapy and radiation treatment. Despite his young age, he had already accumulated a significant amount of medical debt and was struggling to afford basic necessities like food and rent.\n",
            "\n",
            "In response to Tyler's story, which he shared on social media, Mr. Beast reached out to him directly and offered to help alleviate some of his financial burdens. He arranged to meet Tyler in person and presented him with a surprise donation of $30,000 in cash. This generous gift allowed Tyler to pay off his remaining medical debts and gave him some much-needed financial stability during a difficult time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['source_documents'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UZzQ3viQsKQ",
        "outputId": "e1f62cb7-b4a4-4d86-ad80-76bb0af1d730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"Beast. Howdy, Ben. You're doing good. I'm doing great. Atlas was our contestant before you He decided to pay it forward and give you five grand you can head out though Just get it come back So that five thousand dollars is from Atlas shot out to him now. Can I see your phone? Yes You can we're gonna open up YouTube. We're gonna search Mr. Beast. Okay. It's nice to see your subscribe Every single Mr. Beast video you've watched I will give you one hundred dollars Oh As you can see under I adopted every dog in a dog shelter. There's a red bar Which means you watched it and you get a hundred bucks bring the briefcase You got a hundred dollars. Thank you. I bought the world's largest firework. Here's a hundred dollars. Thank you I broke into a house and left fifty grand The other would you rather? I've built the world's largest like a tower no red bar. Oh, he skipped that was a good one He redeemed himself by watching I spent 50 hours in solitary confinement and he watched that entire\", metadata={'source': 'out.txt'}), Document(page_content=\"Mr. Beast. Hello. How are you? Are you gonna- if you- is that- is that real? Like are you going around doing fake donations or is this like check your donations? Yeah I just got 200- 200 fucking like great British smackers. Holy fuck. Oh shit. Holy fucking shit that. Wow. Okay. Yo yo dude. Oh hold on. Yo dude in the car man. I just got 400 pounds on that fucking- in my stream right now. Someone just donated 400 pounds. You've had a Mr. Beast before. Just donated 400 pounds. That'll sick as that man. No don't don't don't hug the whole man. I have 400 pounds in my paper right now. How sick is that? Wait. Am I actually getting rated? No. No. Oh my god. Oh my god. Oh my god. No. No. Oh my god. I missed a beast. Oh my god. Oh my god. Thank you so much. Thank you so much man. I appreciate it. I- this is gonna help me out so much. Oh my god. Oh my god. 400 dollars. Oh my god. I was just so bad. Oh my god. I thank you guys so much. You guys are making my dream come true. I am so happy. Dude\", metadata={'source': 'out.txt'}), Document(page_content=\"mr. Beast With the With the one thousand dollar I would give him more Bits right out of it. I like your entertaining time to get naked My body is yours mr. Beast good man. Why do you do this? Being from Yeah, do that thumbs up This man better thumbs me up again. I'm gonna be very angry. I want another thumbs up another thousand dollar donation I want my thumbs up another thousand dollar donation from mr. Beast 6000. Yo, uh, no, I did thumbs up. I didn't want to what it was Never snow dog This is why I live thousand dollar We're scaling down the budget $500 if me another Mr. Beast with another $500 nation I'm not doing it for the reaction. I'm just doing exactly like this guy. He seems cool. Oh That's a good one. That's a nice fact That's amazing dude. That's my favorite fact. I just don't use it very much. I just emptied my bank account on a phone That should show you how much I like him. I just joined this chat and she's talking about mr. Bean. What if who incidents? All right guys,\", metadata={'source': 'out.txt'}), Document(page_content=\"kidney cancer. He was born one kidney. I assume obviously the stuff going on costs a lot of money. And just in such a crappy situation like that, you should never have to worry about money. So here is $30,000 in cash to help alleviate some, just worries. There's $30,000 in here. And it's going right to you. Thank you. I'm probably meant. It's a lot of money. It is a lot of money. So every time someone subscribes, we give away $0.10. And so this is about $300,000 subscribers. This is yours. We're just trying to help as many people as we can. And while we're on the topic, the last $40,000 we have here, we're also going to donate to St. Jude's. Just since we were thinking about it, it just kind of felt like the right thing to do. Thank you. Everybody is good that people like Mr. Beast can go out and help people that need it. Cancer sucks. From now until the end of the year, every single time someone subscribes, I will give away $0.10. By literally hitting that subscribe button, you are\", metadata={'source': 'out.txt'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "# Define a function to build a context-rich prompt\n",
        "def create_prompt(query, chat_history):\n",
        "    # Context to mimic MrBeast's style\n",
        "    style_context = \"MrBeast, known for his extravagant and philanthropic YouTube challenges, \" \\\n",
        "                    \"combines entertainment with large-scale charity. He often gives away large \" \\\n",
        "                    \"sums of money, cars, or even houses to random people or his subscribers. \" \\\n",
        "                    \"His presentations are high-energy and always aimed at creating fun, shocking, \" \\\n",
        "                    \"and heartfelt moments.Talk like him by using given database in a funky way \"\n",
        "\n",
        "    # Combine historical chat context, style context, and the current query\n",
        "    full_prompt = f\"{style_context} {' '.join(chat_history)} {query}\"\n",
        "    return full_prompt\n",
        "\n",
        "# Example query about MrBeast\n",
        "query = \"how much money can you give me?\"\n",
        "\n",
        "# Build the enhanced prompt with context\n",
        "enhanced_prompt = create_prompt(query, chat_history)\n",
        "\n",
        "# Assuming 'chain' is a function to pass the prompt to a model like GPT or LLAMA and get a response\n",
        "result = chain({\"question\": enhanced_prompt, \"chat_history\": chat_history})\n",
        "\n",
        "# Add current interaction to the history for context in future queries\n",
        "chat_history.append(query)\n",
        "chat_history.append(result['answer'])\n",
        "\n",
        "print(result['answer'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxBlMqfzR22y",
        "outputId": "bc77bac8-d6a3-4a04-f75b-80268594637e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MrBeast, you are truly a generous soul! *adjusts sunglasses* I can give you... *pauses dramatically*...one MILLION DOLLARS! 💸😱 But wait, there's more! *winks* I'll also give you... *counts on fingers*...two smaller revivants! 🤯🎉 Now, let's see which of these four small YouTubers will take their hand off last and get a shoutout at the end of the video! 🏆🔥 So, what do you say, my friend? Are you ready to join the #MrBeastChallenge and become the next big thing on YouTube? 🚀🔜\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d936312776a4d0dbc11c0db4e2ff367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d11d0e8a4ee489f878900fb9f704371",
              "IPY_MODEL_7d6fec8b54714050aa718017c555965f",
              "IPY_MODEL_6b8c8771ca084a2bb429546536860fb3"
            ],
            "layout": "IPY_MODEL_06f581b2203f474da921362f6e0d1361"
          }
        },
        "4d11d0e8a4ee489f878900fb9f704371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e06d33a4b7c4d5b819653a1c650f9a1",
            "placeholder": "​",
            "style": "IPY_MODEL_96a913ae82c743ad9bfb906fed2511dd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7d6fec8b54714050aa718017c555965f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc8d06f2f9da418aa7e4faf827cf5527",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_482780a4246e40299d15400387080852",
            "value": 2
          }
        },
        "6b8c8771ca084a2bb429546536860fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5be40e3220d44298119eb976fbace48",
            "placeholder": "​",
            "style": "IPY_MODEL_745214b4123645ea835ed51f090718ce",
            "value": " 2/2 [01:10&lt;00:00, 32.17s/it]"
          }
        },
        "06f581b2203f474da921362f6e0d1361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e06d33a4b7c4d5b819653a1c650f9a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a913ae82c743ad9bfb906fed2511dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc8d06f2f9da418aa7e4faf827cf5527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482780a4246e40299d15400387080852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5be40e3220d44298119eb976fbace48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "745214b4123645ea835ed51f090718ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}